{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                Version\n",
      "---------------------- -------\n",
      "absl-py                0.7.1  \n",
      "asn1crypto             0.24.0 \n",
      "astor                  0.8.0  \n",
      "attrs                  19.1.0 \n",
      "backcall               0.1.0  \n",
      "bleach                 3.1.0  \n",
      "cloudpickle            1.1.1  \n",
      "cryptography           2.1.4  \n",
      "cycler                 0.10.0 \n",
      "decorator              4.4.0  \n",
      "defusedxml             0.6.0  \n",
      "entrypoints            0.3    \n",
      "enum34                 1.1.6  \n",
      "gast                   0.2.2  \n",
      "google-pasta           0.1.7  \n",
      "grpcio                 1.21.1 \n",
      "h5py                   2.9.0  \n",
      "idna                   2.6    \n",
      "ipykernel              5.1.1  \n",
      "ipython                7.5.0  \n",
      "ipython-genutils       0.2.0  \n",
      "ipywidgets             7.4.2  \n",
      "jedi                   0.13.3 \n",
      "Jinja2                 2.10.1 \n",
      "jsonschema             3.0.1  \n",
      "jupyter                1.0.0  \n",
      "jupyter-client         5.2.4  \n",
      "jupyter-console        6.0.0  \n",
      "jupyter-core           4.5.0  \n",
      "jupyter-http-over-ws   0.0.6  \n",
      "Keras-Applications     1.0.8  \n",
      "Keras-Preprocessing    1.1.0  \n",
      "keyring                10.6.0 \n",
      "keyrings.alt           3.0    \n",
      "kiwisolver             1.1.0  \n",
      "Markdown               3.1.1  \n",
      "MarkupSafe             1.1.1  \n",
      "matplotlib             3.1.0  \n",
      "mistune                0.8.4  \n",
      "nbconvert              5.5.0  \n",
      "nbformat               4.4.0  \n",
      "notebook               5.7.8  \n",
      "numpy                  1.16.4 \n",
      "opt-einsum             3.2.1  \n",
      "pandocfilters          1.4.2  \n",
      "parso                  0.4.0  \n",
      "pexpect                4.7.0  \n",
      "pickleshare            0.7.5  \n",
      "pip                    19.1.1 \n",
      "prometheus-client      0.7.1  \n",
      "prompt-toolkit         2.0.9  \n",
      "protobuf               3.8.0  \n",
      "ptyprocess             0.6.0  \n",
      "pycrypto               2.6.1  \n",
      "Pygments               2.4.2  \n",
      "pygobject              3.26.1 \n",
      "pyparsing              2.4.0  \n",
      "pyrsistent             0.15.2 \n",
      "python-apt             1.6.4  \n",
      "python-dateutil        2.8.0  \n",
      "pyxdg                  0.25   \n",
      "pyzmq                  18.0.1 \n",
      "qtconsole              4.5.1  \n",
      "SecretStorage          2.3.1  \n",
      "Send2Trash             1.5.0  \n",
      "setuptools             41.0.1 \n",
      "six                    1.11.0 \n",
      "tensorboard            1.15.0 \n",
      "tensorflow             1.15.2 \n",
      "tensorflow-estimator   1.15.1 \n",
      "tensorflow-gpu         1.14.0 \n",
      "tensorflow-probability 0.8.0  \n",
      "termcolor              1.1.0  \n",
      "terminado              0.8.2  \n",
      "testpath               0.4.2  \n",
      "tornado                6.0.2  \n",
      "traitlets              4.3.2  \n",
      "wcwidth                0.1.7  \n",
      "webencodings           0.5.1  \n",
      "Werkzeug               0.15.4 \n",
      "wheel                  0.30.0 \n",
      "widgetsnbextension     3.4.2  \n",
      "wrapt                  1.11.2 \n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(paths, labels, nb_samples=None, shuffle=True):\n",
    "    \"\"\"\n",
    "    Takes a set of character folders and labels and returns paths to image files\n",
    "    paired with labels.\n",
    "    Args:\n",
    "        paths: A list of character folders\n",
    "        labels: List or numpy array of same length as paths\n",
    "        nb_samples: Number of images to retrieve per character\n",
    "    Returns:\n",
    "        List of (label, image_path) tuples\n",
    "    \"\"\"\n",
    "    if nb_samples is not None:\n",
    "        sampler = lambda x: random.sample(x, nb_samples)\n",
    "    else:\n",
    "        sampler = lambda x: x\n",
    "    images_labels = [(i, os.path.join(path, image))\n",
    "                     for i, path in zip(labels, paths)\n",
    "                     for image in sampler(os.listdir(path))]\n",
    "    if shuffle:\n",
    "        random.shuffle(images_labels)\n",
    "    return images_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_file_to_array(filename, dim_input):\n",
    "    \"\"\"\n",
    "    Takes an image path and returns numpy array\n",
    "    Args:\n",
    "        filename: Image filename\n",
    "        dim_input: Flattened shape of image\n",
    "    Returns:\n",
    "        1 channel image\n",
    "    \"\"\"\n",
    "    image = imageio.imread(filename)\n",
    "    image = image.reshape([dim_input])\n",
    "    image = image.astype(np.float32) / 255.0\n",
    "    image = 1.0 - image\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(object):\n",
    "    \"\"\"\n",
    "    Data Generator capable of generating batches of Omniglot data.\n",
    "    A \"class\" is considered a class of omniglot digits.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, num_samples_per_class, config={}):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_classes: Number of classes for classification (K-way)\n",
    "            num_samples_per_class: num samples to generate per class in one batch\n",
    "            batch_size: size of meta batch size (e.g. number of functions)\n",
    "        \"\"\"\n",
    "        self.num_samples_per_class = num_samples_per_class\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        data_folder = config.get('data_folder', './omniglot_resized')\n",
    "        self.img_size = config.get('img_size', (28, 28))\n",
    "\n",
    "        self.dim_input = np.prod(self.img_size)\n",
    "        self.dim_output = self.num_classes\n",
    "\n",
    "        character_folders = [os.path.join(data_folder, family, character)\n",
    "                             for family in os.listdir(data_folder)\n",
    "                             if os.path.isdir(os.path.join(data_folder, family))\n",
    "                             for character in os.listdir(os.path.join(data_folder, family))\n",
    "                             if os.path.isdir(os.path.join(data_folder, family, character))]\n",
    "\n",
    "        random.seed(1)\n",
    "        random.shuffle(character_folders)\n",
    "        num_val = 100\n",
    "        num_train = 1100\n",
    "        self.metatrain_character_folders = character_folders[: num_train]\n",
    "        self.metaval_character_folders = character_folders[\n",
    "            num_train:num_train + num_val]\n",
    "        self.metatest_character_folders = character_folders[\n",
    "            num_train + num_val:]\n",
    "\n",
    "    def sample_batch(self, batch_type, batch_size):\n",
    "        \"\"\"\n",
    "        Samples a batch for training, validation, or testing\n",
    "        Args:\n",
    "            batch_type: train/val/test\n",
    "        Returns:\n",
    "            A a tuple of (1) Image batch and (2) Label batch where\n",
    "            image batch has shape [B, K, N, 784] and label batch has shape [B, K, N, N]\n",
    "            where B is batch size, K is number of samples per class, N is number of classes\n",
    "        \"\"\"\n",
    "        if batch_type == \"train\":\n",
    "            folders = self.metatrain_character_folders\n",
    "        elif batch_type == \"val\":\n",
    "            folders = self.metaval_character_folders\n",
    "        else:\n",
    "            folders = self.metatest_character_folders\n",
    "\n",
    "        #############################\n",
    "        #### YOUR CODE GOES HERE ####\n",
    "        labels_images=[]\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            paths=random.sample(folders, self.num_classes)\n",
    "            labels=np.eye(self.num_classes).tolist()\n",
    "            for j in range(self.num_samples_per_class):\n",
    "                labels_images.extend(get_images(paths, labels, 1, shuffle=True))\n",
    "\n",
    "        all_image_batches=[image_file_to_array(label_image[1],784) for label_image in labels_images] \n",
    "        all_label_batches=[label_image[0] for label_image in labels_images]\n",
    "        all_image_batches=np.reshape(all_image_batches,(batch_size, self.num_samples_per_class, self.num_classes, 784))\n",
    "        all_label_batches=np.reshape(all_label_batches, (batch_size, self.num_samples_per_class, self.num_classes, self.num_classes))\n",
    "        #############################\n",
    "\n",
    "        return all_image_batches, all_label_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data_generator(batch_size, num_classes, num_samples_per_class):\n",
    "    \"\"\"\n",
    "        test the DataGenerator class's sample_batch function\n",
    "    Args:\n",
    "        batch size, number of classes, number of samples per class\n",
    "    Returns:\n",
    "        nothing, prints images\n",
    "    \"\"\"\n",
    "    data_generator = DataGenerator(num_classes, num_samples_per_class + 1)\n",
    "    print('Testing data generator for ', num_samples_per_class, 'shot ', num_classes, \\\n",
    "          'way meta-learner with a batch size', batch_size)\n",
    "    train_image_batch, train_label_batch = data_generator.sample_batch('train', batch_size)\n",
    "    print('train image batch shape is ', train_image_batch.shape)\n",
    "    print('train label batch shape is ', train_label_batch.shape)\n",
    "    val_image_batch, val_label_batch = data_generator.sample_batch('val', batch_size)\n",
    "    print('val image batch shape is ', val_image_batch.shape)\n",
    "    print('val label batch shape is ', val_label_batch.shape)\n",
    "    test_image_batch, test_label_batch = data_generator.sample_batch('test', batch_size)\n",
    "    print('test image batch shape is ', test_image_batch.shape)\n",
    "    print('test label batch shape is ', test_label_batch.shape)\n",
    "    return\n",
    "\n",
    "test_data_generator(16,5,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def view_omniglot_imgfiles(n_characters, n_images_pchar, data_folder = './omniglot_resized'):\n",
    "    \"\"\"\n",
    "        view random multiple omniglot image files, for demo purposes\n",
    "    Args:\n",
    "        number of characters, number of images per character, data folder [ex. ./omniglot_resized]\n",
    "    Returns:\n",
    "        nothing, prints images\n",
    "    \"\"\"\n",
    "    character_folders = [os.path.join(data_folder, family, character)\n",
    "                             for family in os.listdir(data_folder)\n",
    "                             if os.path.isdir(os.path.join(data_folder, family))\n",
    "                             for character in os.listdir(os.path.join(data_folder, family))\n",
    "                             if os.path.isdir(os.path.join(data_folder, family, character))]\n",
    "    \n",
    "    paths=random.sample(character_folders, n_characters)\n",
    "    labels=np.eye(n_characters).tolist()\n",
    "    labels_images=get_images(paths, labels, n_images_pchar, shuffle=False)    \n",
    "    image_files =[label_image[1] for label_image in labels_images]\n",
    "\n",
    "    for image_file in image_files:\n",
    "        img=Image.open(image_file)\n",
    "        plt.figure()\n",
    "        plt.imshow(img)\n",
    "    return\n",
    "\n",
    "view_omniglot_imgfiles(10, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "from load_data import DataGenerator\n",
    "#from tensorflow.compat.v1 import flags\n",
    "flags = tf.app.flags \n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "#FLAGS = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    'num_classes', 5, 'number of classes used in classification (e.g. 5-way classification).')\n",
    "\n",
    "flags.DEFINE_integer('num_samples', 1,\n",
    "                     'number of examples used for inner gradient update (K for K-shot learning).')\n",
    "\n",
    "flags.DEFINE_integer('meta_batch_size', 16,\n",
    "                     'Number of N-way classification tasks per batch')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(preds, labels):\n",
    "    \"\"\"\n",
    "    Computes MANN loss\n",
    "    Args:\n",
    "        preds: [B, K+1, N, N] network output\n",
    "        labels: [B, K+1, N, N] labels\n",
    "    Returns:\n",
    "        scalar loss\n",
    "    \"\"\"\n",
    "    #############################\n",
    "    #### YOUR CODE GOES HERE ####\n",
    "    pass\n",
    "    #############################\n",
    "\n",
    "class MANN(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, num_classes, samples_per_class):\n",
    "        super(MANN, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.samples_per_class = samples_per_class\n",
    "        self.layer1 = tf.keras.layers.LSTM(128, return_sequences=True)\n",
    "        self.layer2 = tf.keras.layers.LSTM(num_classes, return_sequences=True)\n",
    "\n",
    "    def call(self, input_images, input_labels):\n",
    "        \"\"\"\n",
    "        MANN\n",
    "        Args:\n",
    "            input_images: [B, K+1, N, 784] flattened images\n",
    "            labels: [B, K+1, N, N] ground truth labels\n",
    "        Returns:\n",
    "            [B, K+1, N, N] predictions\n",
    "        \"\"\"\n",
    "        #############################\n",
    "        #### YOUR CODE GOES HERE ####\n",
    "        pass\n",
    "        #############################\n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ims = tf.placeholder(tf.float32, \\\n",
    "                     shape=(None, FLAGS.num_samples + 1, FLAGS.num_classes, 784))\n",
    "labels = tf.placeholder(tf.float32, \\\n",
    "                        shape=(None, FLAGS.num_samples + 1, FLAGS.num_classes, FLAGS.num_classes))\n",
    "\n",
    "data_generator = DataGenerator(\n",
    "    FLAGS.num_classes, FLAGS.num_samples + 1)\n",
    "\n",
    "o = MANN(FLAGS.num_classes, FLAGS.num_samples + 1)\n",
    "out = o(ims, labels)\n",
    "\n",
    "loss = loss_function(out, labels)\n",
    "optim = tf.train.AdamOptimizer(0.001)\n",
    "optimizer_step = optim.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(50000):\n",
    "        i, l = data_generator.sample_batch('train', FLAGS.meta_batch_size)\n",
    "        feed = {ims: i.astype(np.float32), labels: l.astype(np.float32)}\n",
    "        _, ls = sess.run([optimizer_step, loss], feed)\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(\"*\" * 5 + \"Iter \" + str(step) + \"*\" * 5)\n",
    "            i, l = data_generator.sample_batch('test', 100)\n",
    "            feed = {ims: i.astype(np.float32),\n",
    "                    labels: l.astype(np.float32)}\n",
    "            pred, tls = sess.run([out, loss], feed)\n",
    "            print(\"Train Loss:\", ls, \"Test Loss:\", tls)\n",
    "            pred = pred.reshape(\n",
    "                -1, FLAGS.num_samples + 1,\n",
    "                FLAGS.num_classes, FLAGS.num_classes)\n",
    "            pred = pred[:, -1, :, :].argmax(2)\n",
    "            l = l[:, -1, :, :].argmax(2)\n",
    "            print(\"Test Accuracy\", (1.0 * (pred == l)).mean())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
